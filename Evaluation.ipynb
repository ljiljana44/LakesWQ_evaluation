{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df309930",
   "metadata": {},
   "source": [
    "# Water Quality Prediction from Satellite Data\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db1f99c",
   "metadata": {},
   "source": [
    "# READ CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd00064",
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "# Create parser\n",
    "config = configparser.ConfigParser()\n",
    "\n",
    "# Read config file\n",
    "config.read(\"config.ini\")\n",
    "\n",
    "# Read PATH variables\n",
    "IN_SITU_FILE = config[\"PATHS\"][\"IN_SITU_FILE\"]\n",
    "SENTINEL_DIR = config[\"PATHS\"][\"SENTINEL_DIR\"]\n",
    "LANDSAT_DIR = config[\"PATHS\"][\"LANDSAT_DIR\"]\n",
    "OUTPUT_DIR = config[\"PATHS\"][\"OUTPUT_DIR\"]\n",
    "\n",
    "# Read list variables and convert to Python lists\n",
    "SENTINEL_BAND_NAMES = [\n",
    "    band.strip() \n",
    "    for band in config[\"BANDS\"][\"SENTINEL_BAND_NAMES\"].split(\",\")\n",
    "]\n",
    "\n",
    "LANDSAT_BAND_NAMES = [\n",
    "    band.strip() \n",
    "    for band in config[\"BANDS\"][\"LANDSAT_BAND_NAMES\"].split(\",\")\n",
    "]\n",
    "PARAMS_COL = [\n",
    "    p.strip() \n",
    "    for p in config[\"PARAMS\"][\"PARAMS_COL\"].split(\",\")\n",
    "]\n",
    " \n",
    "\n",
    "# Print to verify\n",
    "print(\"IN_SITU_FILE:\", IN_SITU_FILE)\n",
    "print(\"SENTINEL_BAND_NAMES:\", SENTINEL_BAND_NAMES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11855d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "in_sutu_m = gpd.read_file(IN_SITU_FILE)\n",
    "in_sutu_m['date']=pd.to_datetime(in_sutu_m['DATUM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75b5bc0",
   "metadata": {},
   "source": [
    "## Read Sentinel bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653b9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "\n",
    "base_path = SENTINEL_DIR\n",
    "band_names =  SENTINEL_BAND_NAMES\n",
    "band_names_s=['S_'+b for b in band_names]\n",
    " \n",
    "folder_date_map = {}\n",
    "for f in os.listdir(base_path):\n",
    "    full_path = os.path.join(base_path, f)\n",
    "    if True:#os.path.isdir(full_path):\n",
    "        match = re.search(r\"(\\d{8})\", f)\n",
    "        if match:\n",
    "            if full_path.endswith('.tif'):\n",
    "                date_obj = datetime.strptime(match.group(1), \"%Y%m%d\")\n",
    "                folder_date_map[date_obj] = full_path\n",
    "\n",
    "folder_dates = sorted(folder_date_map.keys())\n",
    "\n",
    " \n",
    "gdf=in_sutu_m.copy()\n",
    "# --- Helper to find nearest available folder date ---\n",
    "def find_nearest_date(target_date, date_list):\n",
    "    return min(date_list, key=lambda d: abs(d - target_date))\n",
    "\n",
    " \n",
    "\n",
    "# Prepare empty columns in gdf\n",
    "for b in band_names_s:\n",
    "    gdf[b] = None\n",
    "def find_nearest_date(target_date, date_list):\n",
    "    return min(date_list, key=lambda d: abs(d - target_date))\n",
    "for idx, row in gdf.iterrows():\n",
    "    #print(row['date'],folder_dates)\n",
    "    nearest_date = find_nearest_date(row['date'], folder_dates)\n",
    "    folder_path = folder_date_map[nearest_date]\n",
    " \n",
    "    \n",
    "\n",
    "    point = row.geometry\n",
    "\n",
    " \n",
    "    for tiff_file in [folder_path]:\n",
    "        # Extract band code from filename (e.g., 'B04')\n",
    "  \n",
    "        with rasterio.open( tiff_file) as src:\n",
    "            if gdf.crs != src.crs:\n",
    "                        point = gdf.to_crs(src.crs).iloc[idx].geometry\n",
    "   \n",
    "            values = list(src.sample([(point.x, point.y)]))[0]\n",
    "            for i in range(len(values)):\n",
    "                 gdf.at[idx, band_names_s[i]] = values[i]\n",
    "                 \n",
    " \n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c257d2a",
   "metadata": {},
   "source": [
    "## Read Landsat bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1efdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import geopandas as gpd\n",
    "import rasterio \n",
    "DATA_DIR= LANDSAT_DIR\n",
    "band_names =  LANDSAT_BAND_NAMES\n",
    "band_names_l=['L_'+b for b in band_names]\n",
    "\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "base_path = DATA_DIR\n",
    " \n",
    "\n",
    "folder_date_map = {}\n",
    "for f in os.listdir(base_path):\n",
    "    full_path = os.path.join(base_path, f)\n",
    "    if True:#os.path.isdir(full_path):\n",
    "        match = re.search(r\"(\\d{8})\", f)\n",
    "        if match:\n",
    "            date_obj = datetime.strptime(match.group(1), \"%Y%m%d\")\n",
    "            folder_date_map[date_obj] = full_path\n",
    "\n",
    "folder_dates = sorted(folder_date_map.keys())\n",
    " \n",
    "def find_nearest_date(target_date, date_list):\n",
    "    return min(date_list, key=lambda d: abs(d - target_date))\n",
    "\n",
    " \n",
    "# Prepare empty columns in gdf\n",
    "for b in band_names_l:\n",
    "    gdf[b] = None\n",
    "\n",
    "\n",
    "def find_nearest_date(target_date, date_list):\n",
    "    return min(date_list, key=lambda d: abs(d - target_date))\n",
    "for idx, row in gdf.iterrows():\n",
    " \n",
    "    nearest_date = find_nearest_date(row['date'], folder_dates)\n",
    "    folder_path = folder_date_map[nearest_date]\n",
    "    \n",
    "\n",
    "    point = row.geometry\n",
    "\n",
    " \n",
    "    for tiff_file in [folder_path]:\n",
    "        \n",
    "        with rasterio.open( tiff_file) as src:\n",
    "   \n",
    "            if gdf.crs != src.crs:\n",
    "                        point = gdf.to_crs(src.crs).iloc[idx].geometry\n",
    "         \n",
    "            values = list(src.sample([(point.x, point.y)]))[0]\n",
    "            for i in range(len(values)):\n",
    "                 gdf.at[idx, band_names_l[i]] = values[i]\n",
    "\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e242416f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "out_dir = Path(OUTPUT_DIR)\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "gdf.to_csv(os.path.join(out_dir, 'dataset_sent_land.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0941ae49",
   "metadata": {},
   "source": [
    "# Read stored csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62149b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "out_vars=PARAMS_COL\n",
    "Landsat_bands=['L_'+b for b in LANDSAT_BAND_NAMES]  \n",
    "Sentinel_bands=['S_'+b for b in SENTINEL_BAND_NAMES]  \n",
    "\n",
    "# read CSV\n",
    "df = pd.read_csv(os.path.join(OUTPUT_DIR, 'dataset_sent_land.csv'))\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cc4145",
   "metadata": {},
   "source": [
    "## Plot Correlation heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c49cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 10))\n",
    "in_X_cols=Sentinel_bands + Landsat_bands\n",
    "out_Y_cols=out_vars\n",
    "\n",
    "corr=df[in_X_cols+ out_Y_cols].corr(method=\"pearson\")\n",
    "# Correlation of features with each target\n",
    " \n",
    "pearson_corr= corr.loc[in_X_cols,out_Y_cols]\n",
    "\n",
    "corr2=df[in_X_cols+ out_Y_cols].corr(method=\"spearman\")\n",
    "\n",
    "spearman_corr= corr2.loc[in_X_cols,out_Y_cols]\n",
    "\n",
    "\n",
    "corr3=df[in_X_cols+ out_Y_cols].corr(method=\"kendall\")\n",
    "\n",
    "kendall_corr= corr3.loc[in_X_cols,out_Y_cols]\n",
    "sns.heatmap(\n",
    "    pearson_corr,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title(\"Pearson Correlation\")\n",
    "\n",
    "sns.heatmap(\n",
    "    spearman_corr,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title(\"Spearman Correlation\")\n",
    "\n",
    "sns.heatmap(\n",
    "    kendall_corr,\n",
    "    annot=True,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    ax=axes[2]\n",
    ")\n",
    "axes[2].set_title(\"Kendall Correlation\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a78aa5",
   "metadata": {},
   "source": [
    "#  Define evaluation framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bd8041",
   "metadata": {},
   "source": [
    "### Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c05686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "bands=Sentinel_bands + Landsat_bands\n",
    "\n",
    "df=df[df['L_B11']>0]\n",
    "inv_cols = [f\"{c}_inv\" for c in bands]\n",
    "df[inv_cols] = df[bands].where(df[bands] != 0).rdiv(1.0)\n",
    "df[inv_cols].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# extend feature list and recompute pearson correlation to include inverses\n",
    " \n",
    "sq_cols = [f\"{c}_sq\" for c in bands]\n",
    "num_cols = [c for c in bands if c in df.columns and np.issubdtype(df[c].dtype, np.number)]\n",
    "\n",
    "df[sq_cols] = df[num_cols].pow(2)\n",
    "df[sq_cols].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "log_cols = [f\"{c}_log\" for c in bands]\n",
    "df[log_cols] = df[num_cols].apply(np.log1p)\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237f4354",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d161b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import (\n",
    "    LinearRegression, Ridge, Lasso, ElasticNet,\n",
    "    HuberRegressor, BayesianRidge\n",
    ")\n",
    "\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    "    ExtraTreesRegressor,\n",
    "    GradientBoostingRegressor,\n",
    "    AdaBoostRegressor,\n",
    "    HistGradientBoostingRegressor\n",
    ")\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# MODELS TO BENCHMARK\n",
    "# ==============================\n",
    "\n",
    "models1 = {\n",
    "\n",
    "    # ----- Linear & Regularized -----\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Ridge Regression\": Ridge(),\n",
    "    \"Lasso Regression\": Lasso(),\n",
    "    \"ElasticNet\": ElasticNet(),\n",
    "    #\"Huber Regressor\": HuberRegressor(),\n",
    "    \"Bayesian Ridge\": BayesianRidge(),\n",
    "\n",
    "    # ----- Polynomial Regression -----\n",
    "    \"Polynomial Regression\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        (\"linear\", LinearRegression())\n",
    "    ]),\n",
    "\n",
    "    # ----- Tree-Based Ensembles -----\n",
    "    \"Random Forest\": RandomForestRegressor(\n",
    "        n_estimators=3, random_state=42, n_jobs=-1,max_depth=3\n",
    "    ),\n",
    "\n",
    "    \"Extra Trees\": ExtraTreesRegressor(\n",
    "        n_estimators=3, random_state=42, n_jobs=-1,max_depth=3\n",
    "    ),\n",
    "\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(\n",
    "        n_estimators=300, random_state=42,max_depth=3\n",
    "    ),\n",
    "\n",
    "    \"HistGradientBoosting\": HistGradientBoostingRegressor( ),\n",
    "\n",
    "    \"AdaBoost\": AdaBoostRegressor(\n",
    "        n_estimators=200, random_state=42\n",
    "    ),\n",
    "\n",
    "    # ----- XGBoost -----\n",
    "    \"XGBoost\": xgb.XGBRegressor(\n",
    "        n_estimators=300,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "\n",
    "    # ----- Kernel Methods -----\n",
    "    \"SVR (RBF)\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"svr\", SVR(kernel=\"rbf\", C=10, epsilon=0.1))\n",
    "    ]),\n",
    "\n",
    "    \"Kernel Ridge (RBF)\": KernelRidge(\n",
    "        kernel=\"rbf\", alpha=1.0, gamma=0.1\n",
    "    ),\n",
    "\n",
    "    # ----- Gaussian Process -----\n",
    "    \"Gaussian Process\": Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"gpr\", GaussianProcessRegressor(\n",
    "        kernel=Matern(nu=1.5),\n",
    "        normalize_y=True,\n",
    "        random_state=42\n",
    "    ))\n",
    "]),\n",
    "\n",
    "    # ----- Distance-Based -----\n",
    "    \"KNN Regressor\": Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"knn\", KNeighborsRegressor(n_neighbors=3))\n",
    "    ]),\n",
    "\n",
    "    # ----- Neural Network -----\n",
    "    \"ANN (MLP)\": MLPRegressor(\n",
    "        hidden_layer_sizes=(64, 32),\n",
    "        activation=\"relu\",\n",
    "        alpha=0.001,\n",
    "        max_iter=2000,\n",
    "        random_state=42\n",
    "    )\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3103e6",
   "metadata": {},
   "source": [
    "## Time Splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29018971",
   "metadata": {},
   "outputs": [],
   "source": [
    "date_splits = [\n",
    "    { \"splt_kind\": \"Random Split (baseline)\"},\n",
    "    \n",
    "\n",
    "    {\n",
    "        # doktorat / projekt\n",
    "        \"splt_kind\": \"Strict temporal projection\",\n",
    "        \"train\": [\n",
    "            \"2023-07-17 00:00:00\",\n",
    "            \"2023-08-18 00:00:00\",\n",
    "            \"2023-09-27 00:00:00\",\n",
    "            \"2023-10-13 00:00:00\",\n",
    "            \"2023-12-04 00:00:00\",\n",
    "            \"2023-12-19 00:00:00\",\n",
    "            \"2024-01-11 00:00:00\",\n",
    "            \"2024-02-19 00:00:00\",\n",
    "            \"2024-03-14 00:00:00\",\n",
    "            \"2024-04-29 00:00:00\",\n",
    "            \"2024-05-24 00:00:00\",\n",
    "            \"2024-06-17 00:00:00\",\n",
    "            \"2024-07-19 00:00:00\",\n",
    "        ],\n",
    "        \"test\": [\n",
    "            \"2025-05-28 00:00:00\",\n",
    "            \"2025-06-26 00:00:00\",\n",
    "            \"2025-07-24 00:00:00\",\n",
    "            \"2025-08-27 00:00:00\",\n",
    "            \"2025-09-16 00:00:00\",\n",
    "            \"2025-10-28 00:00:00\",\n",
    "            \"2025-11-12 00:00:00\",\n",
    "            \"2025-12-12 00:00:00\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    {\n",
    "       \n",
    "        # dok +3 / projekt -3\n",
    "        \"splt_kind\": \"Partial forward projection\",\n",
    "        \"train\": [\n",
    "            \"2023-07-17 00:00:00\",\n",
    "            \"2023-08-18 00:00:00\",\n",
    "            \"2023-09-27 00:00:00\",\n",
    "            \"2023-10-13 00:00:00\",\n",
    "            \"2023-12-04 00:00:00\",\n",
    "            \"2023-12-19 00:00:00\",\n",
    "            \"2024-01-11 00:00:00\",\n",
    "            \"2024-02-19 00:00:00\",\n",
    "            \"2024-03-14 00:00:00\",\n",
    "            \"2024-04-29 00:00:00\",\n",
    "            \"2024-05-24 00:00:00\",\n",
    "            \"2024-06-17 00:00:00\",\n",
    "            \"2024-07-19 00:00:00\",\n",
    "            \"2025-05-28 00:00:00\",\n",
    "            \"2025-06-26 00:00:00\",\n",
    "            \"2025-07-24 00:00:00\",\n",
    "        ],\n",
    "        \"test\": [\n",
    "            \"2025-08-27 00:00:00\",\n",
    "            \"2025-09-16 00:00:00\",\n",
    "            \"2025-10-28 00:00:00\",\n",
    "            \"2025-11-12 00:00:00\",\n",
    "            \"2025-12-12 00:00:00\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    {\n",
    "        # dok +3 last / projekt -3 last\n",
    "        \"splt_kind\": \"Late season calibration\",\n",
    "        \"train\": [\n",
    "            \"2023-07-17 00:00:00\",\n",
    "            \"2023-08-18 00:00:00\",\n",
    "            \"2023-09-27 00:00:00\",\n",
    "            \"2023-10-13 00:00:00\",\n",
    "            \"2023-12-04 00:00:00\",\n",
    "            \"2023-12-19 00:00:00\",\n",
    "            \"2024-01-11 00:00:00\",\n",
    "            \"2024-02-19 00:00:00\",\n",
    "            \"2024-03-14 00:00:00\",\n",
    "            \"2024-04-29 00:00:00\",\n",
    "            \"2024-05-24 00:00:00\",\n",
    "            \"2024-06-17 00:00:00\",\n",
    "            \"2024-07-19 00:00:00\",\n",
    "           \"2025-05-28 00:00:00\",\n",
    "\n",
    "            \"2025-06-26 00:00:00\",\n",
    "            \"2025-07-24 00:00:00\",\n",
    "        ],\n",
    "        \"test\": [\n",
    "            \"2025-08-27 00:00:00\",\n",
    "            \"2025-09-16 00:00:00\",\n",
    "             \"2025-10-28 00:00:00\",\n",
    "            \"2025-11-12 00:00:00\",\n",
    "            \"2025-12-12 00:00:00\",\n",
    "            \n",
    "        ],\n",
    "        },\n",
    "\n",
    "    {\n",
    "        # dok +3 last / projekt -3 last\n",
    "        \"splt_kind\": \"Backward projection\",\n",
    "        \"train\": [\n",
    "          \n",
    "            \"2023-12-04 00:00:00\",\n",
    "            \"2023-12-19 00:00:00\",\n",
    "            \"2024-01-11 00:00:00\",\n",
    "            \"2024-02-19 00:00:00\",\n",
    "            \"2024-03-14 00:00:00\",\n",
    "            \"2024-04-29 00:00:00\",\n",
    "            \"2024-05-24 00:00:00\",\n",
    "            \"2024-06-17 00:00:00\",\n",
    "            \"2024-07-19 00:00:00\",\n",
    "            \"2025-10-28 00:00:00\",\n",
    "            \"2025-11-12 00:00:00\",\n",
    "            \"2025-12-12 00:00:00\",\n",
    "             \"2025-08-27 00:00:00\",\n",
    "            \"2025-09-16 00:00:00\",\n",
    "            \"2025-05-28 00:00:00\",\n",
    "            \"2025-06-26 00:00:00\",\n",
    "            \"2025-07-24 00:00:00\",\n",
    "        ],\n",
    "        \"test\": [\n",
    "           \"2023-07-17 00:00:00\",\n",
    "            \"2023-08-18 00:00:00\",\n",
    "            \"2023-09-27 00:00:00\",\n",
    "            \"2023-10-13 00:00:00\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "    {\n",
    "        # proj + dok -3 / dok -3\n",
    "        \"splt_kind\": \"Extended calibration\",\n",
    "        \"train\": [\n",
    "              \"2023-07-17 00:00:00\",\n",
    "            \"2023-08-18 00:00:00\",\n",
    "            \"2023-09-27 00:00:00\",\n",
    "            \"2023-10-13 00:00:00\",\n",
    "            \"2023-12-04 00:00:00\",\n",
    "            \"2023-12-19 00:00:00\",\n",
    "            \"2024-01-11 00:00:00\",\n",
    "            \"2024-02-19 00:00:00\",\n",
    "            \"2024-03-14 00:00:00\",\n",
    "         \n",
    "            \"2025-05-28 00:00:00\",\n",
    "            \"2025-06-26 00:00:00\",\n",
    "            \"2025-07-24 00:00:00\",\n",
    "            \"2025-08-27 00:00:00\",\n",
    "            \"2025-09-16 00:00:00\",\n",
    "            \"2025-10-28 00:00:00\",\n",
    "            \"2025-11-12 00:00:00\",\n",
    "            \"2025-12-12 00:00:00\",\n",
    "        ],\n",
    "        \"test\": [\n",
    "             \"2024-04-29 00:00:00\",\n",
    "            \"2024-05-24 00:00:00\",\n",
    "            \"2024-06-17 00:00:00\",\n",
    "            \"2024-07-19 00:00:00\",\n",
    "        ],\n",
    "    },\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f166bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "def evaluate_models(models1, X_train, y_train, X_test, y_test, out_Y_cols, splt_kind,feature_names=None):\n",
    "    n_targets = y_train.shape[1]\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    print('-------------------------------------------------')\n",
    "    print('split kind:------',splt_kind)\n",
    "    print(X_train.shape[0],'/',X_test.shape[0])\n",
    "    if feature_names is not None:\n",
    "        print('feature names:------',feature_names)\n",
    " \n",
    "\n",
    "    print(\n",
    "        f\"{'Model':<20} {'Target':<8} \"\n",
    "        f\"{'Features':<20} \"  \n",
    "        f\"{'MAE_tr':>10} {'RMSE_tr':>10} {'R2_tr':>10} \"\n",
    "        f\"{'MAE_te':>10} {'RMSE_te':>10} {'R2_te':>10}\"\n",
    "    )\n",
    "    print(\"-\" * 110)\n",
    "    res=[]\n",
    "\n",
    "    for name, model in models1.items():\n",
    "        try:\n",
    "            for i in range(n_targets):\n",
    "\n",
    "                # IMPORTANT: clone model so each target is independent\n",
    "                m = clone(model)\n",
    "\n",
    "                # ----- FIT -----\n",
    "                m.fit(X_train, y_train[:, i])\n",
    "\n",
    "                # ----- PREDICT -----\n",
    "                y_train_pred = m.predict(X_train).reshape(-1, 1)\n",
    "                y_test_pred  = m.predict(X_test).reshape(-1, 1)\n",
    "\n",
    "    \n",
    "\n",
    "                y_train_orig=y_train[:, i]\n",
    "                y_test_orig=y_test[:, i]\n",
    "            \n",
    "\n",
    "                # ----- METRICS -----\n",
    "                mae_tr  = mean_absolute_error(y_train_orig, y_train_pred)\n",
    "                rmse_tr = np.sqrt(mean_squared_error(y_train_orig, y_train_pred))\n",
    "                r2_tr   = r2_score(y_train_orig, y_train_pred)\n",
    "\n",
    "                mae_te  = mean_absolute_error(y_test_orig, y_test_pred)\n",
    "                rmse_te = np.sqrt(mean_squared_error(y_test_orig, y_test_pred))\n",
    "                r2_te   = r2_score(y_test_orig, y_test_pred)\n",
    "\n",
    "                print(\n",
    "                    f\"{name:<20};  {out_Y_cols[i]:<7};  {splt_kind:<7}; {feature_names:<7};\"\n",
    "                    f\"{mae_tr:10.3f}; {rmse_tr:10.3f}; {r2_tr:10.3f}; \"\n",
    "                    f\"{mae_te:10.3f}; {rmse_te:10.3f}; {r2_te:10.3f}\"\n",
    "                )\n",
    "                res.append([name,out_Y_cols[i],splt_kind,feature_names,mae_tr,rmse_tr,r2_tr, mae_te, rmse_te, r2_te])\n",
    "            \n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{name:<20} ERROR: {e}\")\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdbab79",
   "metadata": {},
   "source": [
    "# Run evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffa9e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "evaluation=[]\n",
    "out_Y_cols=out_vars\n",
    " \n",
    "for in_X_cols, name in [\n",
    "    (Sentinel_bands + Landsat_bands, \"Original\"),\n",
    "    (  inv_cols, \"Inverses\"),\n",
    "    ( sq_cols, \"Squared\"),\n",
    "    (  log_cols, \"Logarithmic\"),\n",
    "]:\n",
    "     df1=df.copy()\n",
    "     df1['date']=pd.to_datetime(df1['DATUM'])\n",
    " \n",
    "      \n",
    "     df1.dropna(subset=in_X_cols + out_Y_cols, inplace=True)\n",
    "\n",
    "     for split in date_splits:\n",
    "        if \"train\" in split and \"test\" in split:\n",
    " \n",
    "            train_dates = pd.to_datetime(split[\"train\"])\n",
    "            test_dates = pd.to_datetime(split[\"test\"])\n",
    "            train_df = df1[df1[\"date\"].isin(train_dates)]\n",
    "            test_df  = df1[df1[\"date\"].isin(test_dates)]\n",
    "            X_train = train_df[in_X_cols].values\n",
    "            y_train = train_df[out_Y_cols].values \n",
    "            X_test = test_df[in_X_cols].values\n",
    "            y_test = test_df[out_Y_cols].values  \n",
    "            print (f\"\\n=== Evaluating with features: {name} ===\")\n",
    "\n",
    "            print(X_train.shape, X_test.shape)\n",
    "            r=evaluate_models(models1, X_train, y_train, X_test, y_test, out_Y_cols,  split[\"splt_kind\"],  name)\n",
    "            evaluation.append(r)\n",
    "        else:\n",
    "            X = df1[in_X_cols].values\n",
    "            y = df1[out_Y_cols].values \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            print (f\"\\n=== Evaluating with features: {name} as {split[\"splt_kind\"]}===\")\n",
    "            r=evaluate_models(models1, X_train, y_train, X_test, y_test, out_Y_cols,  split[\"splt_kind\"],  name)\n",
    "            evaluation.append(r)\n",
    "\n",
    "eval_df = pd.DataFrame(\n",
    "    [item for sublist in evaluation for item in sublist],\n",
    "    columns=[\n",
    "        \"Model\", \"Target\", \"Split\", \"Feature_Set\",\n",
    "        \"MAE_train\", \"RMSE_train\", \"R2_train\",\n",
    "        \"MAE_test\", \"RMSE_test\", \"R2_test\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "eval_df.to_csv(os.path.join(OUTPUT_DIR,\"model_evaluation_results_new.csv\"), index=False)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf14c968",
   "metadata": {},
   "source": [
    "## Use only selected, most correlated bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f1538",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xy_dict = {\n",
    "    \"IRB_EC\": ['S_B01', 'S_B03', 'S_B04', 'S_B09'],\n",
    "    \"IRB_DO\": ['L_B10', 'L_B11'],\n",
    "    \"IRB_TUR\": ['S_B02', 'S_B04', 'S_B07', 'S_B12'],\n",
    "    \"IRB_WT\": ['L_B10', 'L_B11']\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for out_Y , in_X_cols in xy_dict.items():\n",
    " \n",
    "    df1=df.copy()\n",
    "    evaluation=[]\n",
    "    name=f'Selected for {out_Y}'\n",
    "  \n",
    "    out_Y_cols=[out_Y]\n",
    "    df1.dropna(subset=in_X_cols + out_Y_cols, inplace=True)\n",
    "\n",
    "    for split in date_splits:\n",
    "        if \"train\" in split and \"test\" in split:\n",
    "\n",
    "            train_dates = pd.to_datetime(split[\"train\"])\n",
    "            test_dates = pd.to_datetime(split[\"test\"])\n",
    "\n",
    "            train_df = df1[df1[\"date\"].isin(train_dates)]\n",
    "            test_df  = df1[df1[\"date\"].isin(test_dates)]\n",
    "            X_train = train_df[in_X_cols].values\n",
    "            y_train = train_df[out_Y_cols].values  # multivariate regression\n",
    "            X_test = test_df[in_X_cols].values\n",
    "            y_test = test_df[out_Y_cols].values  # multivariate regression\n",
    "            print (f\"\\n=== Evaluating with features: {name} ===\")\n",
    "\n",
    "            print(X_train.shape, X_test.shape)\n",
    "        else:\n",
    "            X = df1[in_X_cols].values\n",
    "            y = df1[out_Y_cols].values \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "            print (f\"\\n=== Evaluating with features: {name} as {split[\"splt_kind\"]}===\")\n",
    "          \n",
    "        r=evaluate_models(models1, X_train, y_train, X_test, y_test, out_Y_cols,  split[\"splt_kind\"],  name)\n",
    "        evaluation.append(r)\n",
    "    eval_df = pd.DataFrame(\n",
    "        [item for sublist in evaluation for item in sublist],\n",
    "        columns=[\n",
    "            \"Model\", \"Target\", \"Split\", \"Feature_Set\",\n",
    "            \"MAE_train\", \"RMSE_train\", \"R2_train\",\n",
    "            \"MAE_test\", \"RMSE_test\", \"R2_test\"\n",
    "        ]\n",
    "    )\n",
    "\n",
    "  \n",
    "    eval_df.to_csv(os.path.join(OUTPUT_DIR,f'model_evaluation_results_new{out_Y_cols[0]}.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
